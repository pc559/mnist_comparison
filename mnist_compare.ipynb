{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorporated-bryan",
   "metadata": {},
   "source": [
    "Classifying handwritten digits\n",
    "-----\n",
    "The MNIST dataset contains **labelled images of handwritten digits** - one common machine learning exercise\n",
    "is to use this dataset to train a model to recognise such digits. In this write-up we will see why - it's\n",
    "a difficult task but the methods give impressive results.\n",
    "\n",
    "One of my personal goals here is to understand what all the fuss is about with\n",
    "neural networks, by comparing them to other models in the context of this task.\n",
    "In the course of implementing these models,\n",
    "we'll take a look at **hyperparameter tuning**,\n",
    "and some basic methods for **reducing the dimensionality** of the dataset.\n",
    "\n",
    "Results (edit this!!)\n",
    "-------\n",
    "After testing multiple models we chose the two that performed best on our validation set.\n",
    "Those two models are MLPClassifier and XGBClassifier;\n",
    "the former using dimensionally reduced features from polynomial decomposition,\n",
    "the latter with no dimensional reduction.\n",
    "Using the test set to estimate their error rate, we found\n",
    "that XGBClassifier had an accuracy of 97.1%,\n",
    "while MLPClassifier had an accuracy of 97.5%.\n",
    "\n",
    "The neural net is the clear winner here. It converged far faster than the boosted decision trees,\n",
    "and was far more accurate than the linear SVC model. There are definitely caveats to this however - for example,\n",
    "I didn't explore variations to the neural network architecture, I only varied very few of the parameters of the\n",
    "other models, and for the sake of training time I reduced the size of the training data that the linear svc had\n",
    "access to.\n",
    "\n",
    "Legendre polynomial decomposition slightly outperformed PCA as a method of dimensional reduction for this example;\n",
    "PCA on the other hand has the obvious advantage that it is well known and easily included in the Sklearn model pipeline.\n",
    "\n",
    "Metrics\n",
    "----\n",
    "We will use the Sklearn accuracy_score as our scoring metric.\n",
    "This is the **fraction of the samples that were correctly classified**. \n",
    "Any quoted times will be **wall clock times** as measured on my laptop - note these timings are not meant to be rigorous comparisons!\n",
    "The loss function used by MLPClassifier is the **cross-entropy**, which acts to find the network parameters that maximise the likelihood of the training data.\n",
    "\n",
    "Models\n",
    "-----\n",
    "We will compare the following models in terms of both accuracy and speed. It won't be a totally\n",
    "fair comparison, because (as specified below) we won't actually train all of these methods on the entire\n",
    "training set, to keep computation time about even (in terms of wall clock time, not cpu-time).\n",
    "1. **MLPClassifier**, a multi-layer perceptron (MLP). We will use a simple neural net with only one hidden layer.\n",
    "2. **XGBoost** is a gradient boosted ensemble of decision trees. It is optimised to take advantage of the computational resources available - it definitely has impressive CPU usage on my laptop!\n",
    "3. **LinearSVC**, a support vector classifier with a linear kernel.\n",
    "4. **GradientBoostingClassifier**, another gradient boosted ensemble of decision trees.\n",
    "\n",
    "Preprocessing for dimensional reduction\n",
    "------\n",
    "The first bit of preprocessing that we (always) do is standardising our features,\n",
    "by removing the mean and scaling to unit variance. But beyond that, we will attempt to\n",
    "increase the signal-to-noise of our data by applying methods of dimensional reduction.\n",
    "Why would we want to reduce the dimensionality of our dataset? Surely losing data is a bad thing?\n",
    "One primary reason is **robustness to overfitting** - if we can drop\n",
    "some of the noise in our training data, without losing signal, we will improve the generality of our model.\n",
    "There are three types of dimensional reduction that we will look at here:\n",
    "1. **None** - This dataset is small enough that we can just train on all the data, so let's try that. How well does this work, does it overfit on the noise?\n",
    "2. **Principal component analysis** - PCA extracts the features that explain the majority of the variance. How well do these features function as predictors for the digits? \n",
    "3. **Polynomial decomposition** - We can fit the pixel image with a (2D) basis of Legendre polynomials. How well do the coefficients of this fit work as predictors?\n",
    "\n",
    "There are two reasons for choosing the Legendre polynomials in particular over other possible basis sets. The first is that they are **orthogonal** on the interval [-1,1] and so the errors of the two-dimensional version should behave well on our pixel squares. The second reason is personal - I have experience with them from my PhD, so I'm curious to see how they perform here! See **\"Visualising the Legendre basis\"** at the end of this notebook to see what the basis actually looks like. We'll use **Nmax** to refer to the maximum order of the polynomials that we'll include, so the two dimensional products will form a total of Nmax^2 basis polynomials. We get one coefficient for each of these basis polynomials, so **Nmax^2 features in total**.\n",
    "\n",
    "There are multiple important differences between using PCA components as features and using the polynomial coefficients as features.\n",
    "For one thing, the PCA will converge to a (near-)perfect reconstruction of the\n",
    "image as the number of components approaches the dimension of the data,\n",
    "whereas the polynomial decomposition would require many more terms.\n",
    "This is because, **unlike PCA, the polynomial basis is not decided by the training set**.\n",
    "This may mean that the basis fits the data less efficiently,\n",
    "but it should also increase robustness to noise.\n",
    "The polynomial decomposition will also be especially bad at reconstructing sharp edges,\n",
    "so it will (in effect) smooth the pixel image.\n",
    "However, **the processed features do not actually have to reconstruct the original features well** - they just have to preserve the information that distinguishes the digits.\n",
    "\n",
    "Confidence checking\n",
    "-----\n",
    "A large fraction of the time spent learning a new tool is actually spent\n",
    "learning to **debug** that tool!\n",
    "When the model doesn't do what we expect, we need to know what to check.\n",
    "In the spirit of that, we will implement\n",
    "some of the confidence checks mentioned in the excellent blog post\n",
    "https://karpathy.github.io/2019/04/25/recipe/\n",
    ".\n",
    "* **Exploring the data** - What does the data look like? What is the distribution of labels? After a model is trained, are there any obvious patterns in the mislabelled samples?\n",
    "* **Randomising the labels** - How does our model behave when trained on labels that are randomised? This dataset will be all noise and no signal, so will be a measure of the ability of our model to simply memorise labels or spot imbalances in our data.\n",
    "* **Plotting the loss curve** - How does the loss evolve as the model trains? Does it decrease, as it should? Any large spikes from outlier batches?\n",
    "\n",
    "To do\n",
    "-----\n",
    "* TRIM!!!\n",
    "* Do final run to get somewhat-fair times\n",
    "* Correct imbalance\n",
    "* Implement better early stopping\n",
    "* Implement model_selection.KFold, GridSearchCV\n",
    "* What is the state of the art for MNIST?\n",
    "* What is the state of the art for orthogonal polynomial basis sets?\n",
    "* Unsupervised MNIST, how many real classes? (eg seven vs crossed seven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "protected-survivor",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "from numpy.random import permutation\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.integrate import quad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.special import eval_legendre\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "np.random.seed(42)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-application",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # If you want to run this notebook, you'll need to have a local copy of the MNIST dataset.\n",
    "name = 'mnist_784'\n",
    "mnist = fetch_openml(name, data_home='~/scikit_learn_data/openml/'+name)\n",
    "N1 = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-fault",
   "metadata": {},
   "source": [
    "Data exploration\n",
    "----\n",
    "\n",
    "Let's see what kind of data we're dealing with! We'll plot the images of the first few digits, and check the count of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-brother",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,6,figsize=(12,6))\n",
    "axs = axs.flatten()\n",
    "N_plot = len(axs)\n",
    "for i in range(N_plot):\n",
    "    axs[i].imshow(mnist.data.values[i].reshape((N1,N1)))\n",
    "plt.show()\n",
    "print('Max:', np.max(mnist.data.values))\n",
    "print('Min:', np.min(mnist.data.values))\n",
    "print('Shape:', np.shape(mnist.data.values))\n",
    "print('Labels and counts:', Counter(mnist['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-guess",
   "metadata": {},
   "source": [
    "The dataset consists of 70000 greyscale images. The images are encoded as 28 by 28 pixels, taking values between 0 and 255. The labels are characters representing the digits 0 to 9, and we can see that there is a **slight imbalance in the labels** in the dataset - there are more 1's and fewer 5's, but the counts are all around 7000.\n",
    "\n",
    "Train/validation/test split\n",
    "-----\n",
    "Let's split up the dataset for training and testing. Two sets won't suffice, however - we'll also need a set for hyperparameter tuning, which we'll call the validation set.\n",
    "We'll split the dataset up into a **training set** with **half** the data, and a **validation set** and **test set** with a **quarter each**.\n",
    "The hyperparameter we'll tune is the number of features that are fed into the model.\n",
    "We'll pick the best value of this hyperparameter using the validation set.\n",
    "This means that the validation set cannot give us a true estimate of the error, since it will be biased to seem\n",
    "more accurate than it actually is.\n",
    "When we evaluate the score on the test set, at the very end, that will give us a true\n",
    "**estimate of the error** of the model.\n",
    "\n",
    "Let's print the counts for the three sets. Ideally we would have a perfectly even split of labels in the training set\n",
    "so that the model doesn't accidentally learn things like \"twos are more common than fives\". It's not impossible\n",
    "that we will want to use this model in a situation where that is actually true (eg if Benfordâ€™s law was in play)\n",
    "but for the sake of this notebook anyway, we will aim to weight the accuracy of each digit equally.\n",
    "\n",
    "If the label distribution in the training set is uneven,\n",
    "then at the very least we don't want it to be uneven in the same way that the validation and\n",
    "test sets are uneven - this would increase the risk of the model overfitting to the distribution,\n",
    "artificially inflating our test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-draft",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = mnist['data'].values\n",
    "y = mnist['target']\n",
    "test_size = 0.5\n",
    "X_train, X_nottrain, y_train, y_nottrain = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_nottrain, y_nottrain, test_size=0.5, random_state=43)\n",
    "for grp in [X_train, X_valid, X_test]:\n",
    "    print(np.shape(grp))\n",
    "for grp in [y_train, y_valid, y_test]:\n",
    "    print('Labels and counts:', Counter(grp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-tsunami",
   "metadata": {},
   "source": [
    "The sets are **imbalanced** - we should probably fix this, but for this notebook we'll just keep an eye\n",
    "on 5 in the tests on the validation set below, to make sure it isn't disadvantaged too much.\n",
    "\n",
    "Model parameters\n",
    "-------\n",
    "\n",
    "The main model we'll test is a basic MLP with **a single hidden layer** with **120 nodes**. We'll use rectified linear units (ReLU's) as activation functions. We choose our learning rate to be 1e-4.\n",
    "We won't vary the network architecture in this notebook, as varying the architecture\n",
    "in addition to comparing with other methods would make things a bit messy.\n",
    "I chose 120 nodes after briefly testing a few possibilities on the training set, and the learning rate was chosen to be small\n",
    "enough to eliminate spikes in the loss curves (caused by outlier batches fed into the stochastic weight optimizer used in the fit).\n",
    "We're not going to vary too far from the defaults for the other comparison models, for the sake of keeping this notebook simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-force",
   "metadata": {},
   "source": [
    "(If you want to speedily run this notebook, eg for debugging purposes, you should lower each N_samples_cut value and increase the tols in the code below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-franklin",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mlp_tol = 1e-4\n",
    "svc_tol = 1e-3\n",
    "## # It takes ages to train on the randomly permuted labels,\n",
    "## # so let's make the tolerance smaller.\n",
    "rand_tol = 1e-2\n",
    "\n",
    "## # Nmax is the maximum polynomial order in the Legendre decomposition,\n",
    "## # these are the values we will check. The number of factors for a\n",
    "## # given Nmax is Nmax^2.\n",
    "Nmax_to_check = [9, 10, 11, 12, 13, 14, 16, 28]#range(8,29,4)\n",
    "\n",
    "hidden_layer_sizes = [120]\n",
    "method_name, method = 'mlp', MLPClassifier\n",
    "method_params = {'hidden_layer_sizes':hidden_layer_sizes, 'activation':'relu', 'random_state':16, 'tol':mlp_tol, 'learning_rate_init':1e-4, 'max_iter':4000}\n",
    "rand_params  = {'hidden_layer_sizes':hidden_layer_sizes, 'activation':'relu', 'random_state':16, 'tol':rand_tol, 'learning_rate_init':1e-4, 'max_iter':4000}\n",
    "\n",
    "#N_samples_cut = {LinearSVC.__name__:100, XGBClassifier.__name__:350, GradientBoostingClassifier.__name__:100}\n",
    "N_samples_cut = {LinearSVC.__name__:10000, XGBClassifier.__name__:35000, GradientBoostingClassifier.__name__:10000}\n",
    "other_methods = [XGBClassifier, LinearSVC, GradientBoostingClassifier]\n",
    "other_params = dict()\n",
    "other_params[LinearSVC.__name__] = {'max_iter':4000, 'dual':False, 'tol':svc_tol}\n",
    "XGB_early_stopping_rounds = 10\n",
    "other_params[XGBClassifier.__name__] = {'use_label_encoder':False, \"objective\":\"multi:softmax\",\"num_class\":10, \"eval_metric\":\"mlogloss\"}\n",
    "## # n_iter_no_change needs to be set!\n",
    "other_params[GradientBoostingClassifier.__name__] = {}#{'tol':tol}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-growth",
   "metadata": {},
   "source": [
    "MLPClassifier with PCA\n",
    "----\n",
    "First, we'll use **PCA** to pick out a set of features which should hopefully capture the data well. This will help by reducing the noise, making the model more robust to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-equity",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pca_results = []\n",
    "display_cols = ['Components','Validation score','Train score','Time taken [s]']\n",
    "## # This is a bit innefficient as the PCA need only be done once,\n",
    "## # and the first n_components taken at each iteration\n",
    "## # (but this way lets us use the pipeline set-up more easily).\n",
    "for sqrt_n_components in Nmax_to_check:\n",
    "    n_components = sqrt_n_components**2\n",
    "    t1 = time()\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    pca_pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca), (method_name, method(**method_params))])\n",
    "    pca_pipe.fit(X_train, y_train)\n",
    "    t2 = time()\n",
    "    \n",
    "    score_valid = accuracy_score(y_valid, pca_pipe.predict(X_valid))\n",
    "    score_train = accuracy_score(y_train, pca_pipe.predict(X_train))\n",
    "    pca_results.append([int(n_components), score_valid, score_train, t2-t1])\n",
    "    print(n_components, end='\\t')\n",
    "    #print(n_components, score_valid, score_train, t2-t1, sep='\\t', flush=True)\n",
    "print('')\n",
    "pca_results = np.array(pca_results)\n",
    "pca_scores = pd.DataFrame(pca_results, columns=display_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-decline",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pca_scores['Method'] = 'MLPClassifier'\n",
    "pca_scores['Prep'] = 'PCA'\n",
    "pca_scores['Samples'] = len(X_train)\n",
    "pca_scores['Components'] = pca_scores['Components'].astype(np.int64)\n",
    "pca_scores.style.set_caption(\"PCA with MLPClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-metadata",
   "metadata": {},
   "source": [
    "That's pretty good! The training score is perfect, which could be a warning sign of overfitting.\n",
    "We check this by looking at the validation score, which peaks around 97.2% with 144 features.\n",
    "This means that the model is correctly labelling 97.2% of the 17,500 validation samples correctly,\n",
    "and so has not suffered too much from overfitting.\n",
    "\n",
    "Let's take a look at the fraction of each class which was correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-brunei",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "con = confusion_matrix(y_valid, pca_pipe.predict(X_valid), normalize='true')\n",
    "diag_acc = np.diag(con)/np.sum(con, axis=1)\n",
    "ranked = sorted(zip(pca_pipe.classes_, diag_acc), key=lambda x:x[1], reverse=True)\n",
    "print('Class\\tAccuracy')\n",
    "for cl, sc in ranked:\n",
    "    print(cl, round(sc,2), sep='\\t')\n",
    "#plt.imshow(con)\n",
    "#plt.title('Confusion matrix')\n",
    "#plt.show()\n",
    "#for a in con:\n",
    "#    for b in a:\n",
    "#        print(round(b,3), end='\\t')\n",
    "#    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-airplane",
   "metadata": {},
   "source": [
    "The classification accuracy seems pretty even, it doesn't look like the model is concentrating too much on any particular digit.\n",
    "The digit 1 may have benefitted from being the most represented in both the training and validation sets, but it doesn't look like\n",
    "5 suffered too much from being the rarest in both.\n",
    "\n",
    "Now let's take a look at the **loss curve**, to see what the training trajectory looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-society",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pca_pipe[method_name].loss_curve_)\n",
    "plt.title('Full dataset loss curve')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-investigator",
   "metadata": {},
   "source": [
    "The loss curve is smooth, giving us some hope that we have chosen our learning rate correctly.\n",
    "It doesn't look like the loss curve has bottomed out, so we may be able to increase performance by training for longer,\n",
    "but then we would run the risk of overfitting.\n",
    "\n",
    "MLPClassifier without dimensional reduction\n",
    "----\n",
    "What if we train the same MLP using the **full dataset**, without applying any dimensional reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-filter",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # Full\n",
    "n_components = X_valid.shape[1]\n",
    "#print(n_components)\n",
    "t1 = time()\n",
    "full_pipe = Pipeline([('scaler', StandardScaler()), (method_name, method(**method_params))])\n",
    "full_pipe.fit(X_train, y_train)\n",
    "t2 = time()\n",
    "score_valid = accuracy_score(y_valid, full_pipe.predict(X_valid))\n",
    "score_train = accuracy_score(y_train, full_pipe.predict(X_train))\n",
    "full_result = [int(n_components), score_valid, score_train, t2-t1]\n",
    "display_cols = ['Components','Validation score','Train score','Time taken [s]']\n",
    "full_score = pd.DataFrame([full_result], columns=display_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-banks",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wrong_inds = y_valid!=full_pipe.predict(X_valid)\n",
    "print('Misclassifications:', len(y_valid[wrong_inds]), '\\n', Counter(y_valid[wrong_inds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-burst",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "full_score['Method'] = 'MLPClassifier'\n",
    "full_score['Prep'] = 'None'\n",
    "full_score['Samples'] = len(X_train)\n",
    "full_score['Components'] = X_train.shape[1]\n",
    "full_score.style.set_caption(\"MLPClassifier with no dimensional reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-editor",
   "metadata": {},
   "source": [
    "The first thing we notice is that training MLPClassifier with the full data, with no preprocessing,\n",
    "performs quite well. It is about 0.5% lower than the best PCA result however - this is presumably because dropping\n",
    "the latter PCA components reduces the noise in the training data, decreasing the variance of the resulting model.\n",
    "\n",
    "We also notice that this model does perform better than PCA with 784 components, the number of features in\n",
    "the initial training set. This is strange, as in theory the PCA reconstruction with a full complement of\n",
    "features should just be a linear combination of the same information content, so we would expect the model\n",
    "to converge to the same result. One possibility is slight numerical inaccuracies due to large cancellations\n",
    "between the PCA components.\n",
    "The PCA does reconstruct the images well, despite very large cancellations,\n",
    "as we see in the test image at the end of this notebook.\n",
    "However it might still be possible that these cancellations could be an issue for the neural net.\n",
    "I would like to chase this further, but for the sake of keeping this notebook from expanding\n",
    "too much, I'll leave this mystery here.\n",
    "\n",
    "Comparison models without dimensional reduction\n",
    "----\n",
    "Let's compare **XGBClassifier**, **LinearSVC** and **GradientBoostingClassifier**.\n",
    "The latter two are quite slow in comparison with the MLP, so **we will only train on a subset of the data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-charity",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "other_results = []\n",
    "for mthd in other_methods:\n",
    "    n_components = X_valid.shape[1]\n",
    "    t1 = time()\n",
    "    mthd_pipe = Pipeline([('scaler', StandardScaler()), (mthd.__name__, mthd(**other_params[mthd.__name__]))])\n",
    "    cut = N_samples_cut[mthd.__name__]\n",
    "    print(f'Using {cut} training samples for {mthd.__name__}.', flush=True)\n",
    "    if mthd.__name__=='XGBClassifier':\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(y_train[:cut].astype(\"category\"))\n",
    "        mthd_pipe.fit(X_train[:cut], label_encoder.transform(y_train[:cut].astype(\"category\")))\n",
    "    else:\n",
    "        mthd_pipe.fit(X_train[:cut], y_train[:cut])\n",
    "    t2 = time()\n",
    "    score_valid, score_train = 0., 0.\n",
    "    if mthd.__name__=='XGBClassifier':\n",
    "        score_valid = accuracy_score(label_encoder.transform(y_valid[:cut]), mthd_pipe.predict(X_valid[:cut]))\n",
    "        score_train = accuracy_score(label_encoder.transform(y_train[:cut]), mthd_pipe.predict(X_train[:cut]))\n",
    "    else:\n",
    "        score_valid = accuracy_score(y_valid[:cut], mthd_pipe.predict(X_valid[:cut]))\n",
    "        score_train = accuracy_score(y_train[:cut], mthd_pipe.predict(X_train[:cut]))\n",
    "    other_results.append([mthd.__name__, cut, score_valid, score_train, t2-t1])\n",
    "display_cols = ['Method','Samples','Validation score','Train score','Time taken [s]']\n",
    "other_methods_scores = pd.DataFrame(other_results, columns=display_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-exception",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "other_methods_scores['Prep'] = 'None'\n",
    "other_methods_scores['Components'] = X_train.shape[1]\n",
    "other_methods_scores.style.set_caption(\"Comparison models, no dimensional reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-collect",
   "metadata": {},
   "source": [
    "Wow, XGBClassifier is impressive! It took about a factor of four longer than the PCA MLP took to train,\n",
    "but the result is slightly better, at 97.4% vs 97.2%.\n",
    "\n",
    "No signal, all noise\n",
    "------\n",
    "Let's take a quick meander to put the above results in context. Have our models really learned anything?\n",
    "We will explore this question by training the same MLP using the full dataset, with the target **labels randomly permuted**.\n",
    "There should be **no signal** here, the entire training set should be noise.\n",
    "If our model can achieve a good training score, that would mean our model is capable of extremely overfitting the data.\n",
    "By comparing with this example, we can have confidence that when using the correctly labelled data,\n",
    "our model is **really learning** something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-blowing",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # Random (no signal, all noise)\n",
    "n_components = X_valid.shape[1]\n",
    "t1 = time()\n",
    "rand_pipe = Pipeline([('scaler', StandardScaler()), (method_name, method(**rand_params))])\n",
    "permed_train = permutation(y_train)\n",
    "rand_pipe.fit(X_train, permed_train)\n",
    "t2 = time()\n",
    "score_valid = accuracy_score(y_valid, rand_pipe.predict(X_valid))\n",
    "score_train = accuracy_score(permed_train, rand_pipe.predict(X_train))\n",
    "rand_result = [n_components, score_valid, score_train, t2-t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-start",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Counter of predicted labels:\\n',Counter(rand_pipe.predict(X_valid)))\n",
    "display_cols = ['Components','Validation score','Train score','Time taken [s]']\n",
    "pd.DataFrame([rand_result], columns=display_cols).style.set_caption(\"Randomised labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-logging",
   "metadata": {},
   "source": [
    "The training score of 22.5% is due to the ability of the model to overfit the data. As expected, the validation score is around 10%,\n",
    "reflecting the even distribution of the ten classes in the training and validation sets - **the model is essentially guessing**.\n",
    "It should also be kept in mind that we have set the tolerance lower here than in the other examples, to reduce \n",
    "computation time - I got similar validation scores with lower tolerance, though the training score can be higher.\n",
    "\n",
    "This context makes the results of our models seem even more impressive - they really are learning something,\n",
    "they really are picking up on some meaningful signal in the data.\n",
    "\n",
    "MLPClassifier with polynomial decomposition\n",
    "----\n",
    "Let's try using the coefficients of a **2D Legendre polynomial decomposition** as features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-penguin",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "N_pixel=28\n",
    "breaks = np.linspace(-1,1,N_pixel+1,endpoint=True)\n",
    "centers = np.array([np.mean(breaks[i:i+2]) for i in range(N_pixel)])\n",
    "\n",
    "## # Calculating the matrix Q that maps pixels->coefficients.\n",
    "Q = np.zeros((N_pixel, N_pixel))\n",
    "for n in range(N_pixel):\n",
    "    for i in range(N_pixel):\n",
    "        Q[n,i] = quad(lambda x,n=n:eval_legendre(n, x)*np.sqrt(n+0.5), breaks[i], breaks[i+1], epsabs=1.49e-08, epsrel=1.49e-08, limit=50)[0]\n",
    "\n",
    "leg_features_sq = []\n",
    "X_train_sq = X_train.reshape((-1,28,28))\n",
    "X_valid_sq = X_valid.reshape((-1,28,28))\n",
    "## # Obtaining the Legendre coefficients of each image to use as features\n",
    "for to_fit in X_train_sq:\n",
    "    coeffs_full = np.einsum('ia,ba->ib', Q, to_fit)\n",
    "    coeffs_full = np.einsum('jb,ib->ij', Q, coeffs_full)\n",
    "    leg_features_sq.append(coeffs_full)\n",
    "\n",
    "leg_results = []\n",
    "for Nmax in Nmax_to_check:\n",
    "    t1 = time()\n",
    "    leg_features = np.array([cs[:Nmax,:Nmax].flatten() for cs in leg_features_sq])\n",
    "    \n",
    "    ## # Fitting the model\n",
    "    leg_pipe = Pipeline([('scaler', StandardScaler()), (method_name, method(**method_params))])\n",
    "    leg_pipe.fit(leg_features, y_train)\n",
    "    t2 = time()\n",
    "    \n",
    "    ## # Converting the validation set (this should be pulled outside the loop!)\n",
    "    leg_valid = []\n",
    "    for to_fit in X_valid_sq:\n",
    "        coeffs = np.einsum('ia,ba->ib', Q[:Nmax], to_fit)\n",
    "        coeffs = np.einsum('jb,ib->ij', Q[:Nmax], coeffs)\n",
    "        leg_valid.append(coeffs.flatten())\n",
    "    leg_valid = np.array(leg_valid)\n",
    "    \n",
    "    ## # Scoring\n",
    "    score_valid = accuracy_score(y_valid, leg_pipe.predict(leg_valid))\n",
    "    score_train = accuracy_score(y_train, leg_pipe.predict(leg_features))\n",
    "    leg_results.append([Nmax**2, score_valid, score_train, t2-t1])\n",
    "    print(Nmax**2, end='\\t')\n",
    "print('')\n",
    "display_cols = ['Components','Validation score','Train score','Time taken [s]']\n",
    "leg_scores = pd.DataFrame(leg_results, columns=display_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-integer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "leg_scores['Method'] = 'MLPClassifier'\n",
    "leg_scores['Prep'] = 'poly'\n",
    "leg_scores['Samples'] = len(X_train)\n",
    "leg_scores.style.set_caption(\"Polynomial decomp with MLPClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-deposit",
   "metadata": {},
   "source": [
    "By eye, the scores and times for polynomial decomposition preprocessing (labelled by \"poly\") seem similar to the PCA case - we'll\n",
    "need to plot them to really get a feel for the difference.\n",
    "\n",
    "PCA vs polynomial decomposition\n",
    "----\n",
    "Let's compare them properly, by plotting the scores and times as we increase the numbers of features.\n",
    "We'll also plot the loss curves for PCA and Legendre polynomial preprocessing with 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-combine",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18,6))\n",
    "axs[0].plot([r[0] for r in leg_results], [1-r[1] for r in leg_results], '-x', label='Valid poly')\n",
    "axs[0].plot([r[0] for r in leg_results], [1-r[2] for r in leg_results], '-x', label='Train poly')\n",
    "axs[1].plot([r[0] for r in leg_results], [r[3]/60 for r in leg_results], '-x', label='poly')\n",
    "axs[0].plot([r[0] for r in pca_results], [1-r[1] for r in pca_results], '-x', label='Valid pca')\n",
    "axs[0].plot([r[0] for r in pca_results], [1-r[2] for r in pca_results], '-x', label='Train pca')\n",
    "axs[1].plot([r[0] for r in pca_results], [r[3]/60 for r in pca_results], '-x', label='pca')\n",
    "xmin, xmax = min(pca_results[:,0]), max(pca_results[:,0])\n",
    "axs[0].hlines(1-full_result[1], xmin, xmax, linestyles='--', label='Valid full')\n",
    "axs[0].hlines(1-full_result[2], xmin, xmax, linestyles='-.', label='Train full')\n",
    "axs[1].hlines(full_result[3]/60, xmin, xmax, linestyles='--', label='full')\n",
    "axs[2].plot(pca_pipe[method_name].loss_curve_, '-', label='PCA')\n",
    "axs[2].plot(full_pipe[method_name].loss_curve_, '-', label='Full')\n",
    "axs[2].plot(rand_pipe[method_name].loss_curve_[:len(leg_pipe[method_name].loss_curve_)], '-', label='Rand (truncated)')\n",
    "axs[2].plot(leg_pipe[method_name].loss_curve_, '-', label='poly')\n",
    "axs[2].set_yscale('log')\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[2].legend()\n",
    "axs[0].set_title('Training and Validation Score')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Training Loss Curve')\n",
    "axs[0].set_xlabel('Features')\n",
    "axs[0].set_ylabel('Fraction incorrect')\n",
    "axs[1].set_xlabel('Features')\n",
    "axs[1].set_ylabel('Time [m]')\n",
    "axs[2].set_ylabel('Loss')\n",
    "axs[2].set_xlabel('Iterations')\n",
    "axs[0].set_ylim(-0.005, 0.05)\n",
    "axs[1].set_ylim(0, 3.)\n",
    "plt.show()\n",
    "print(f\"Final value of rand loss: {len(rand_pipe[method_name].loss_curve_)} iterations gives loss={round(rand_pipe[method_name].loss_curve_[-1],2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-parish",
   "metadata": {},
   "source": [
    "The validation score curves show us that the polynomial features are more **robust against overfitting**,\n",
    "as the score keeps improving to a larger number of features (around 200, vs around 150 for PCA features).\n",
    "**Spikes in the loss curves** are due to having a learning rate that is too large.\n",
    "We have reduced our learning rate here to **1e-4** to remove these spikes, which are due to outlier minibatches.\n",
    "The plots show there is **no significant difference in time taken to train**,\n",
    "though this isn't a totally fair comparison - the PCA is being performed for every iteration of the loop, when in reality it\n",
    "only needs to be performed once, and the first n components taken.\n",
    "\n",
    "Using all the features leads to overfitting. PCA helps, but since the features are picked based on variance they might not actually hold the cluster information. Polynomial coefficients as features cut out noise, but can also make use of more features before giving in to overfitting, hence achieve a better validation score (though the difference is only marginal, 97.7% compared to 97.2%).\n",
    "\n",
    "Comparison models, with polynomial decomposition\n",
    "----\n",
    "What about training our comparison models (XGBClassifier, LinearSVC, GradientBoostingClassifier) on the polynomial features,\n",
    "choosing the number of polynomial features that gave us the best result with MLPClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-whale",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_sq = X_train.reshape((-1,28,28))\n",
    "X_valid_sq = X_valid.reshape((-1,28,28))\n",
    "Nmax_sq = leg_scores['Components'][leg_scores['Validation score']==leg_scores['Validation score'].max()].values[0]\n",
    "Nmax = int(np.sqrt(Nmax_sq))\n",
    "print(f'Maximum at Nmax={Nmax}, so {Nmax_sq} components.')\n",
    "\n",
    "## # Using the same pixel->coeff matrix Q calculated above\n",
    "## # Training set:\n",
    "leg_train = []\n",
    "for to_fit in X_train_sq:\n",
    "    coeffs = np.einsum('ia,ba->ib', Q[:Nmax], to_fit)\n",
    "    coeffs = np.einsum('jb,ib->ij', Q[:Nmax], coeffs)\n",
    "    leg_train.append(coeffs.flatten())\n",
    "leg_train = np.array(leg_train)\n",
    "\n",
    "## # Validation set:\n",
    "leg_valid = []\n",
    "for to_fit in X_valid_sq:\n",
    "    coeffs = np.einsum('ia,ba->ib', Q[:Nmax], to_fit)\n",
    "    coeffs = np.einsum('jb,ib->ij', Q[:Nmax], coeffs)\n",
    "    leg_valid.append(coeffs.flatten())\n",
    "leg_valid = np.array(leg_valid)\n",
    "    \n",
    "#######\n",
    "\n",
    "leg_other_results = []\n",
    "for mthd in other_methods:\n",
    "    n_components = leg_valid.shape[1]\n",
    "    mthd_pipe = Pipeline([('scaler', StandardScaler()), (mthd.__name__, mthd(**other_params[mthd.__name__]))])\n",
    "    cut = N_samples_cut[mthd.__name__]\n",
    "    print(f'Using {cut} training samples for {mthd.__name__}.', flush=True)\n",
    "    t1 = time()\n",
    "    if mthd.__name__=='XGBClassifier':\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(y_train[:cut].astype(\"category\"))\n",
    "        mthd_pipe.fit(leg_train[:cut], label_encoder.transform(y_train[:cut].astype(\"category\")))\n",
    "    else:\n",
    "        mthd_pipe.fit(leg_train[:cut], y_train[:cut])\n",
    "    t2 = time()\n",
    "    score_valid, score_train = 0., 0.\n",
    "    if mthd.__name__=='XGBClassifier':\n",
    "        score_valid = accuracy_score(label_encoder.transform(y_valid[:cut]), mthd_pipe.predict(leg_valid[:cut]))\n",
    "        score_train = accuracy_score(label_encoder.transform(y_train[:cut]), mthd_pipe.predict(leg_train[:cut]))\n",
    "    else:\n",
    "        score_valid = accuracy_score(y_valid[:cut], mthd_pipe.predict(leg_valid[:cut]))\n",
    "        score_train = accuracy_score(y_train[:cut], mthd_pipe.predict(leg_train[:cut]))\n",
    "    leg_other_results.append([mthd.__name__, cut, score_valid, score_train, t2-t1])\n",
    "display_cols = ['Method', 'Samples', 'Validation score', 'Train score', 'Time taken [s]']\n",
    "other_methods_scores_leg = pd.DataFrame(leg_other_results, columns=display_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-translator",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "other_methods_scores_leg['Prep'] = 'poly'\n",
    "other_methods_scores_leg['Components'] = leg_train.shape[1]\n",
    "other_methods_scores_leg.style.set_caption(\"Polynomial decomp with comparison models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-collectible",
   "metadata": {},
   "source": [
    "Some interesting results here!\n",
    "\n",
    "**XGBClassifier** took a little longer, and got an accuracy score 0.2% lower,\n",
    "which we can assume is due to dropping from 784 components to 196.\n",
    "**LinearSVC** has vastly improved, however - around thirty times faster, and an accuracy score increase\n",
    "of around 5%! An accuracy of 0.91 is still quite low compared to the other methods, but given the vast increase\n",
    "in training speed we now could try increasing the number of training samples from 10000 to the full training set.\n",
    "**GradientBoostingClassifier** has performed about the same as it did with all the features, taking slightly longer\n",
    "here but achieving a slightly better accuracy.\n",
    "\n",
    "Ranking the methods\n",
    "----\n",
    "Let's rank all the methods we've tried so far! We'll only include the best PCA example and the best polynomial decomposition example in\n",
    "the following table, which we will order by validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-reproduction",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # Change this to a plot?\n",
    "col1 = 'Method'\n",
    "col2 = 'Prep'\n",
    "pca_inds = pca_scores['Validation score']==pca_scores['Validation score'].max()\n",
    "leg_inds = leg_scores['Validation score']==leg_scores['Validation score'].max()\n",
    "dfs = [other_methods_scores, other_methods_scores_leg, pca_scores[pca_inds], leg_scores[leg_inds], full_score]\n",
    "temp = pd.concat(dfs)\n",
    "temp['Components'] = temp['Components'].astype(np.int64)\n",
    "temp['Time taken [s]'] = np.round(temp['Time taken [s]'])\n",
    "temp = temp.set_index([col1, col2], drop=True).sort_values('Validation score', ascending=False)\n",
    "temp.style.set_caption(\"All models, ranked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-filling",
   "metadata": {},
   "source": [
    "**Add note about tol being different!!**\n",
    "**Compare loss curves?**\n",
    "\n",
    "The results are clear - the two models that were computationally efficient enough to take advantage of the full\n",
    "training set, **MLPClassifier and XGBClassifier**, perform a few percent better than GradientBoostingClassifier and LinearSVC.\n",
    "All of the models performed better with **dimensional reduction**, except XGBClassifier which managed to extract the signal itself\n",
    "from the raw (standardised) data. For the MLPClassifier, the polynomial coefficients performed better as features than the PCA\n",
    "components did, however only by 0.5% (and of course, PCA has the advantage of being very widely known and easily\n",
    "accessible in the standard libraries).\n",
    "\n",
    "Based on these results, we will choose two models; **MLPClassifier with the polynomial dimensional reduction** (14X14 components)\n",
    "and **XGBClassifier, with no preprocessing** (beyond standard scaling, of course).\n",
    "\n",
    "And now for the grand finale - testing our chosen models on our test set, to estimate their true error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-framework",
   "metadata": {},
   "source": [
    "Grand finale\n",
    "-----\n",
    "To state something incredibly obvious, the accuracy score on the training set is not going to be a good estimate for the model's\n",
    "**accuracy on unseen data**, as this is the data the model was, well, trained on.\n",
    "Similarly, the accuracy score on the validation set won't be a good estimate of the error on unseen data either - this is the set\n",
    "we used to choose our hyperparameters, so the error will be **biased** to be smaller than the true error.\n",
    "This is why **we have kept a quarter of our data locked away** up until now. Now that all our tuning is done\n",
    "we can finally apply our model to that data and estimate the **true error rate** of the model.\n",
    "Once we've seen this, we can't tune our model any further and still have an honest estimate of our error rate, so we better be sure we're ready!\n",
    "\n",
    "First, **XGBClassifier with no dimensional reduction**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-punch",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # XGBoost, None, on test\n",
    "\n",
    "XGB_test_result = []\n",
    "\n",
    "XGB_pipe = Pipeline([('scaler', StandardScaler()), (XGBClassifier.__name__, XGBClassifier(**other_params[XGBClassifier.__name__]))])\n",
    "cut = N_samples_cut[XGBClassifier.__name__]\n",
    "print(f'Using {cut} training samples for {XGBClassifier.__name__}.', flush=True)\n",
    "t1 = time()\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y_train[:cut].astype(\"category\"))\n",
    "XGB_pipe.fit(X_train[:cut], label_encoder.transform(y_train[:cut].astype(\"category\")))\n",
    "t2 = time()\n",
    "score_test, score_train = 0., 0.\n",
    "score_test = accuracy_score(label_encoder.transform(y_test[:cut]), XGB_pipe.predict(X_test[:cut]))\n",
    "score_train = accuracy_score(label_encoder.transform(y_train[:cut]), XGB_pipe.predict(X_train[:cut]))\n",
    "XGB_test_result.append([XGBClassifier.__name__, cut, score_test, score_train, t2-t1, 'None', X_train.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-treaty",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_test_encoded = np.array(label_encoder.transform(y_test[:cut].astype(\"category\")))\n",
    "#print(np.shape(y_test_encoded))\n",
    "#print(np.shape(XGB_pipe.predict(X_test)))\n",
    "wrong_inds = y_test_encoded!=XGB_pipe.predict(X_test[:cut])\n",
    "print(f'Misclassifications: {len(y_test[:cut][wrong_inds])}\\n',Counter(y_test[:cut][wrong_inds]))\n",
    "display_cols = ['Method', 'Samples', 'Test score', 'Train score', 'Time taken [s]', 'Prep', 'Components']\n",
    "XGB_test_result_scores = pd.DataFrame(XGB_test_result, columns=display_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-hotel",
   "metadata": {},
   "source": [
    "And second, **MLPClassifier using polynomial decomposition**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-concert",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # MLP, Legendre, on test\n",
    "N_opt = int(np.sqrt(leg_scores['Components'][leg_inds]))\n",
    "print(f'Using Nmax={N_opt}')\n",
    "\n",
    "X_train_sq = X_train.reshape((-1,28,28))\n",
    "X_test_sq = X_test.reshape((-1,28,28))\n",
    "leg_train = []\n",
    "for to_fit in X_train_sq:\n",
    "    coeffs = np.einsum('ia,ba->ib', Q[:N_opt], to_fit)\n",
    "    coeffs = np.einsum('jb,ib->ij', Q[:N_opt], coeffs)\n",
    "    leg_train.append(coeffs.flatten())\n",
    "leg_train = np.array(leg_train)\n",
    "\n",
    "## # Test set:\n",
    "leg_test = []\n",
    "for to_fit in X_test_sq:\n",
    "    coeffs = np.einsum('ia,ba->ib', Q[:N_opt], to_fit)\n",
    "    coeffs = np.einsum('jb,ib->ij', Q[:N_opt], coeffs)\n",
    "    leg_test.append(coeffs.flatten())\n",
    "leg_test = np.array(leg_test)\n",
    "    \n",
    "#######\n",
    "\n",
    "MLP_test_result = []\n",
    "\n",
    "n_components = leg_test.shape[1]\n",
    "MLP_pipe = Pipeline([('scaler', StandardScaler()), (MLPClassifier.__name__, MLPClassifier(**method_params))])\n",
    "t1 = time()\n",
    "MLP_pipe.fit(leg_train, y_train)\n",
    "t2 = time()\n",
    "score_test, score_train = 0., 0.\n",
    "score_test = accuracy_score(y_test, MLP_pipe.predict(leg_test))\n",
    "score_train = accuracy_score(y_train, MLP_pipe.predict(leg_train))\n",
    "MLP_test_result.append([MLPClassifier.__name__, X_train.shape[0], score_test, score_train, t2-t1, 'poly', leg_train.shape[1]])\n",
    "\n",
    "display_cols = ['Method', 'Samples', 'Test score', 'Train score', 'Time taken [s]', 'Prep', 'Components']\n",
    "MLP_test_result_score = pd.DataFrame(MLP_test_result, columns=display_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-polymer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wrong_inds = y_test!=MLP_pipe.predict(leg_test)\n",
    "print(f'Misclassifications: {len(y_test[wrong_inds])}\\n',Counter(y_test[wrong_inds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-excuse",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([XGB_test_result_scores, MLP_test_result_score]).style.set_caption(\"Results on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-whale",
   "metadata": {},
   "source": [
    "So that's our final result: the estimated accuracy of our XGBClassifier model is 97.1%, which is slightly\n",
    "outperformed by the 97.5% achieved by our\n",
    "MLPClassifier (dimensionally reduced using polynomial decomposition). Both of these are a little lower\n",
    "than the respective scores on the validation set - a good reminder of why it's important to keep a\n",
    "separate testing set locked away for exactly this purpose!\n",
    "\n",
    "Conclusions\n",
    "-----\n",
    "Our neural net performed pretty well! Using only the most basic architecture it managed to (slightly) outperform\n",
    "our XGBoost model in accuracy, and far outperform it in speed of training. The neural net didn't achieve this on\n",
    "its own, however - by taking a polynomial decomposition of the original image and providing those coefficients as\n",
    "features for the model, it was able to ignore the noise and focus on fitting the real signal.\n",
    "\n",
    "I'm sure there are many ways we could improve all of these implementations - systemised experimentation using\n",
    "hyperparameter grid searches, and using cross-validation instead of hold-out being two examples.\n",
    "However, my motivations in writing this notebook were simply to convince myself that neural networks are impressive,\n",
    "and to compare PCA and Legendre polynomial decomposition as methods of dimensional reduction, and the results we have obtained\n",
    "achieve those goals.\n",
    "\n",
    "Thank you for reading this far! See below for some visualisations of PCA vs Legendre polynomial decompositions,\n",
    "if you're curious.\n",
    "\n",
    "Bonus - visualising the processed features\n",
    "---\n",
    "\n",
    "How well do the Legendre polynomial/PCA decompositions capture the image?\n",
    "Recall that the processed features do not actually have to be able to reconstruct the image perfectly,\n",
    "as they only need sufficient infomation to distinguish between the digits. In fact, it is\n",
    "preferable if they can't reconstruct the noise!\n",
    "Still, let's take a look at what the reconstructions actually look like.\n",
    "\n",
    "In the following images the **colour scheme is not standardised!**\n",
    "To add a metric for the amplitude of what's being plotted, I've added the variance of the pixels in the title of\n",
    "each image and scaled the target image to have unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-carbon",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # 2d\n",
    "N_pixel = 28\n",
    "breaks = np.linspace(-1,1,N_pixel+1,endpoint=True)\n",
    "centers = np.array([np.mean(breaks[i:i+2]) for i in range(N_pixel)])\n",
    "mnist_ind = 53\n",
    "## # Careful to copy, not view!\n",
    "to_fit_example = np.copy(mnist['data'].values[mnist_ind].reshape((28,28)))\n",
    "to_fit_example -= to_fit_example.mean()\n",
    "to_fit_example = to_fit_example/to_fit_example.std()\n",
    "def func(x, y):\n",
    "    x = np.clip(np.ceil((1+x)*N_pixel/2), 0, 27)\n",
    "    y = np.clip(np.ceil((1+y)*N_pixel/2), 0, 27)\n",
    "    temp = to_fit_example[y.astype(np.int64), x.astype(np.int64)]\n",
    "    return temp\n",
    "\n",
    "N_plot = 150\n",
    "xs = np.linspace(-1, 1, N_plot, endpoint=True)\n",
    "ys = np.linspace(-1, 1, N_plot, endpoint=True)\n",
    "f_xs, f_ys = np.meshgrid(xs, ys)[0].flatten(), np.meshgrid(xs, ys)[1].flatten()\n",
    "fns = func(f_xs, f_ys)\n",
    "\n",
    "fig, axs = plt.subplots(3, 5, figsize=(12,6))\n",
    "axs = axs.flatten()\n",
    "recon_res = []\n",
    "for ind, Nmax in enumerate(range(2, 29, 2)):#range(1,32,10):\n",
    "    coeffs = np.einsum('ia,ba->ib', Q[:Nmax], to_fit_example)\n",
    "    coeffs = np.einsum('jb,ib->ij', Q[:Nmax], coeffs)\n",
    "    recomp = np.dot(coeffs, [eval_legendre(n, xs) for n in range(Nmax)])\n",
    "    recomp = np.dot(recomp.T, [eval_legendre(n, xs) for n in range(Nmax)]).flatten()\n",
    "    recon_res.append([Nmax, np.sum((recomp-fns)**2)/np.sum(fns**2)])\n",
    "    scale = (28./N_plot)**2 # Relative number of pixels\n",
    "    axs[ind].set_title(f'poly {Nmax}, variance={round(np.var(recomp),2)}')\n",
    "    #axs[ind].set_title(f'poly {Nmax}, log_sq_sum={round(np.log10(scale*np.sum(recomp**2)),2)}')\n",
    "    axs[ind].imshow(recomp.reshape((N_plot,N_plot)), label='recomp', extent=[-1, 1, -1, 1])\n",
    "    print(Nmax, end='\\t', flush=True)\n",
    "    axs[ind].axis('off')\n",
    "recon_res = np.array(recon_res)\n",
    "axs[-1].set_title(f'Original (resampled),\\nvariance={round(np.var(fns),2)}')\n",
    "axs[-1].axis('off')\n",
    "axs[-1].imshow(fns.reshape((N_plot,N_plot)), extent=[-1, 1, -1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig, ax = plt.subplots(1,1,figsize=(6,3))\n",
    "#ax.set_title('Legendre reconstruction error')\n",
    "#ax.plot(recon_res[:,0]**2, recon_res[:,1], '-x')\n",
    "#ax.set_ylabel('Error')\n",
    "#ax.set_xlabel('Components')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-assignment",
   "metadata": {},
   "source": [
    "The Legendre polynomial reconstruction manages to make the reconstruction error\n",
    "small, but never zero. This is to be expected,\n",
    "as they could never reconstruct the sharp edges of the pixels. Despite this, we can see from the images that\n",
    "they do provide a good representation of the written digit.\n",
    "\n",
    "Now let's see what the same reconstruction looks like for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-dynamics",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## # Plot the coefficients of a particular image.\n",
    "#fig, axs = plt.subplots(1, 3, figsize=(18,6))\n",
    "#axs[0].set_title('Original')\n",
    "#axs[1].set_title(f'Nmax={Nmax}')\n",
    "#axs[2].set_title('Coefficients')\n",
    "#axs[0].imshow(fns.reshape((N_plot, N_plot)), label='real', extent=[-1, 1, -1, 1])\n",
    "#axs[1].imshow(recomp.reshape((N_plot, N_plot)), label='recomp', extent=[-1, 1, -1, 1])\n",
    "#axs[2].imshow(coeffs[::-1,:], extent=[0, Nmax, 0, Nmax])\n",
    "#plt.show()\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-library",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 5, figsize=(12,6))\n",
    "axs = axs.flatten()\n",
    "pca_recon = []\n",
    "for ind, sqrt_n_components in enumerate(range(2, 29, 2)):\n",
    "    ## # Should rewrite to do full once, then just use the first n.\n",
    "    pca = PCA(n_components=sqrt_n_components**2, random_state=42)\n",
    "    pca_only_pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "    pca_only_pipe.fit(X_train)\n",
    "    pca_fit = pca_only_pipe.transform(to_fit_example.reshape((1,-1)))\n",
    "    pca_inv_fit = pca_only_pipe.inverse_transform(pca_fit).reshape((28,28))\n",
    "    axs[ind].set_title(f'PCA {sqrt_n_components**2}, variance={round(np.var(pca_inv_fit),2)}')\n",
    "    #axs[ind].set_title(f'PCA {sqrt_n_components**2}, log_sq_sum={round(np.log10(np.sum(pca_inv_fit**2)),2)}')\n",
    "    axs[ind].imshow(pca_inv_fit, label='recomp', extent=[-1, 1, -1, 1])\n",
    "    print(sqrt_n_components, end='\\t', flush=True)\n",
    "    axs[ind].axis('off')\n",
    "    pca_recon.append([sqrt_n_components**2, np.sum((pca_inv_fit-to_fit_example)**2)/np.sum(to_fit_example**2)])\n",
    "pca_recon = np.array(pca_recon)\n",
    "print('')\n",
    "axs[-1].set_title(f'Original, variance={round(np.var(to_fit_example),2)}')\n",
    "axs[-1].axis('off')\n",
    "axs[-1].imshow(to_fit_example, label='recomp', extent=[-1, 1, -1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-memphis",
   "metadata": {},
   "source": [
    "Wow, there is a big difference in how PCA and Legendre polynomials reconstruct the image! The Legendre polynomials always have a reasonable, human-readable, approximation to the final result. Even if they could never efficiently capture the sharp edges of the pixels,\n",
    "the magnitude of the the residuals is never much larger than the pixel values themselves.\n",
    "The PCA features, on the other hand, have gigantic residuals!\n",
    "\n",
    "We can quantify this by plotting the reconstruction error, which we will define as the mean square residual,\n",
    "divided by the mean square pixel value of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-spirituality",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#print(pca_recon[-2,0], round(pca_recon[-2,1],2), sep=': ')\n",
    "#print(pca_recon[-1,0], round(pca_recon[-1,1],2), sep=': ')\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,3))\n",
    "ax.set_title('Reconstruction errors')\n",
    "ax.plot(recon_res[:,0]**2, recon_res[:,1], '-x', label='poly')\n",
    "ax.plot(pca_recon[:,0], pca_recon[:,1], '-x', label='PCA')\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('Components')\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-3, 1e3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'Final poly residual: {round(recon_res[-1,1],2)}')\n",
    "print(f'Final PCA residual: {pca_recon[-1,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-criminal",
   "metadata": {},
   "source": [
    "The convergence is far less smooth for PCA than for the Legendre case,\n",
    "and only when the number of components nears the full 784 does the reconstruction error beat the Legendre case,\n",
    "becoming effectively perfect.\n",
    "\n",
    "Visualising the Legendre polynomial basis\n",
    "----\n",
    "What do the two-dimensional Legendre polynomial basis functions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-norway",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#print(coeffs[:3,:3])\n",
    "#print(coeffs[-3:,-3:])\n",
    "N_plot = 101\n",
    "ps = np.linspace(-1, 1, N_plot, endpoint=True)\n",
    "xs, ys = np.meshgrid(ps, ps)[0].flatten(), np.meshgrid(ps, ps)[1].flatten()\n",
    "Nmax_plot = 7\n",
    "fig, axs = plt.subplots(Nmax_plot, Nmax_plot, figsize=(6,6))\n",
    "for n1 in range(Nmax_plot):\n",
    "    temp = eval_legendre(n1, xs)*np.sqrt(n1+0.5)\n",
    "    for n2 in range(Nmax_plot):\n",
    "        temp2 = temp*eval_legendre(n2, ys)*np.sqrt(n2+0.5)\n",
    "        #axs[n1][n2].set_title(f'{n1}, {n2}')\n",
    "        axs[n1][n2].imshow(temp2.reshape((N_plot,N_plot)))\n",
    "        axs[n1][n2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-unemployment",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-canada",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-graduation",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
